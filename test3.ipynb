{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "import sqlalchemy as sa\n",
    "import getpass\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "\n",
    "ENGLISH_WORDS = set(words.words())\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current time:  2023-06-18 16:09:14\n"
     ]
    }
   ],
   "source": [
    "from app.setup import setup\n",
    "connector = setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfModel:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def preprocess_text(self, docs):\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens_list = []\n",
    "        for doc in docs:\n",
    "            tokens = nltk.word_tokenize(doc)\n",
    "            tokens = [token.lower() for token in tokens if token.lower() not in stop_words and token.isalpha() and token.lower() in ENGLISH_WORDS]\n",
    "            tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "            tokens_list.append(tokens)\n",
    "        return tokens_list\n",
    "\n",
    "    def cal_tfidf(self, docs):\n",
    "        tfidf = TfidfVectorizer()\n",
    "        self.feature_vectors = tfidf.fit_transform(docs)\n",
    "        self.sim_matrix = cosine_similarity(self.feature_vectors)\n",
    "\n",
    "    def get_feature_vectors(self, ids=[]):\n",
    "        if ids:\n",
    "            result_vectors = []\n",
    "            for id in ids:\n",
    "                result_vectors.append(self.feature_vectors[self.indices[id]])\n",
    "            return (ids, sparse.vstack(result_vectors))\n",
    "        else:\n",
    "            return (self.indices.index, self.feature_vectors)\n",
    "\n",
    "    def update_features(self, new_matrix):\n",
    "        self.user_profiles = None\n",
    "        self.feature_vectors = new_matrix\n",
    "        self.sim_matrix = cosine_similarity(self.feature_vectors)\n",
    "\n",
    "    def get_user_profile(self, items, ratings, alpha=1):\n",
    "        self.user_items = items\n",
    "        _, self.items_vectors = self.get_feature_vectors(self.user_items)\n",
    "        self.user_ratings = ratings\n",
    "        ridge = Ridge(alpha=alpha).fit(self.items_vectors, self.user_ratings)\n",
    "        return (ridge, sparse.csr_matrix(ridge.coef_))\n",
    "\n",
    "    def fit_users(self, user_data, id_col, item_col, rating_col, top=100):\n",
    "        # self.reduce_features()\n",
    "        self.user_data = user_data\n",
    "        user_list = user_data[id_col].unique().tolist()\n",
    "        user_profiles = []\n",
    "        self.user_indices = pd.Series(range(0,len(user_list)),index=user_list)\n",
    "        total = len(user_list)\n",
    "        for index, user in enumerate(user_list):\n",
    "            print(f'fitting {index+1}/{top}.')\n",
    "            this_user = user_data.loc[user_data[id_col] == user]\n",
    "            _, profile = self.get_user_profile(this_user[item_col].astype(str).tolist(),\n",
    "                                            this_user[rating_col].astype(str).tolist(), 100)\n",
    "            user_profiles.append(profile)\n",
    "            clear_output(wait=True)\n",
    "            if index + 1 == top:\n",
    "                break\n",
    "        self.user_profiles = sparse.vstack(user_profiles)\n",
    "        self.user_matrices = cosine_similarity(self.user_profiles)\n",
    "\n",
    "    def get_personal_recommendations(self, id, n_users=5, n_items=10):\n",
    "        this_user = self.user_data.loc[self.user_data[0] == id]\n",
    "        clf, _ = self.get_user_profile(this_user[1].astype(str).tolist(),\n",
    "                                       this_user[2].astype(str).tolist(), 100)\n",
    "        user_items = self.user_data[1].loc[self.user_data[0] == id].tolist()\n",
    "        a = []\n",
    "        for iid in self.indices.index.tolist():\n",
    "            if iid not in self.user_items:\n",
    "                a.append(iid)\n",
    "        ids, remained = self.get_feature_vectors(a)        \n",
    "        predicts = clf.predict(remained).flatten()\n",
    "        pre_dict = dict(zip(ids, predicts))\n",
    "        pre_dict = dict(sorted(pre_dict.items(), key=lambda item: item[1],reverse=True)[:n_items])\n",
    "\n",
    "        idx = self.user_indices[id]\n",
    "        sim_scores = list(enumerate(self.user_matrices[idx]))\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        sim_scores = sim_scores[1:n_users + 1]\n",
    "        sim_users_indices = [i[0] for i in sim_scores]\n",
    "        sim_users = self.user_indices[sim_users_indices].tolist()\n",
    "        pi = []\n",
    "        for user in sim_users:\n",
    "            items = self.user_data.loc[self.user_data[0] == user]\n",
    "            like_items = items[1].loc[items[2] >= 4].tolist()\n",
    "            pi = pi + like_items[:n_items]\n",
    "        pi = list(set(pi))\n",
    "        recs = [str(item) for item in pi if item not in user_items]\n",
    "        return (list(pre_dict.keys()), recs)\n",
    "\n",
    "\n",
    "    def score(self, measurement, test_data, test_ratings):\n",
    "        _, test_vectors = self.get_feature_vectors(test_data)\n",
    "        predicts = self.ridge.predict(test_vectors).flatten()\n",
    "        score = measurement(predicts, test_ratings)\n",
    "        return score\n",
    "\n",
    "    def evaluatePR(self, test_data, top=5):\n",
    "        tp = 0\n",
    "        recommended_items = self.get_personal_recommendations(top)\n",
    "        for item in recommended_items:\n",
    "            if item in test_data:\n",
    "                tp += 1\n",
    "        return tp\n",
    "\n",
    "    def get_recommendations(self, id, top=5):\n",
    "        idx = self.indices[id]\n",
    "        sim_scores = list(enumerate(self.sim_matrix[idx]))\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        sim_scores = sim_scores[1:top + 1]\n",
    "        item_indices = [i[0] for i in sim_scores]\n",
    "        return self.data[self.id].iloc[item_indices].tolist()\n",
    "\n",
    "    def reduce_features(self):\n",
    "        parameters = {\n",
    "            'n_clusters': None,\n",
    "            'metric': 'euclidean',\n",
    "            'linkage': 'ward',\n",
    "            'distance_threshold': 1.38,\n",
    "            'compute_distances': True\n",
    "        }\n",
    "        print('reducing features!')\n",
    "        model = AgglomerativeClustering(\n",
    "            **parameters).fit(self.feature_vectors.toarray())\n",
    "        labels = model.labels_\n",
    "        x_new = SelectKBest(chi2, k=4000).fit_transform(\n",
    "            self.feature_vectors, labels)\n",
    "        self.update_features(x_new)\n",
    "        clear_output()\n",
    "\n",
    "    def fit(self, data, id, docs, is_feature_reduced=False):\n",
    "        self.indices = pd.Series(data.index, index=data[id])\n",
    "        print('initializing features')\n",
    "        tokens_list = [[' '.join(doc)][0]\n",
    "                       for doc in self.preprocess_text(data[docs])]\n",
    "        self.cal_tfidf(tokens_list)\n",
    "        clear_output()\n",
    "        if is_feature_reduced:\n",
    "            self.reduce_features()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qbooks = sa.text(\"select * from books;\")\n",
    "books = pd.read_sql_query(qbooks, con=connector.connect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/main/ratings_train_1.dat',sep=\":\",header=None)\n",
    "test = pd.read_csv('./data/main/ratings_test_1.dat',sep=\":\",header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user(user):\n",
    "    user_train = train.loc[train[0] == user]\n",
    "    user_test = test.loc[test[0] == user]\n",
    "    return (user_train, user_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_id = 'goodreads_book_id'\n",
    "books_docs = 'description'\n",
    "model = TfidfModel()\n",
    "model.fit(books, books_id, books_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting 100/100.\n"
     ]
    }
   ],
   "source": [
    "model.fit_users(train, 0, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec0, rec1 = model.get_personal_recommendations(10049, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15196',\n",
       " '80834',\n",
       " '353814',\n",
       " '11391817',\n",
       " '25106',\n",
       " '8694',\n",
       " '11553',\n",
       " '7775841',\n",
       " '5128',\n",
       " '16640']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12127810, 5349, 6, 55401, 7005865, 7664041, 7090447, 17487, 102868, 12820793]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
